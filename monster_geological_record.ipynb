{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3524b8-a538-4c0c-90ed-c1ddcc2a537d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gprm import ReconstructionModel\n",
    "import pygmt\n",
    "import geopandas as gpd\n",
    "import sys\n",
    "sys.path.append('/Users/simon/GIT/degenerative_art/')\n",
    "import map_effects as me\n",
    "import xarray as xr\n",
    "from xrspatial import proximity\n",
    "import gprm.utils.paleogeography as pg\n",
    "import pygplates\n",
    "from gprm.utils.create_gpml import gpml2gdf\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f085f677-6fe7-439d-ae24-6cfcdefcfff5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Atlantis = ReconstructionModel()\n",
    "\n",
    "Atlantis.add_continent_polygons('/Users/simon/Documents/2022IMAS-OUC_SOMG/PracFiles/Atlantis2/Atlantis2_continents.gpml')\n",
    "Atlantis.add_dynamic_polygons('/Users/simon/Documents/2022IMAS-OUC_SOMG/PracFiles/Atlantis2/Atlantis2_topologies.gpml')\n",
    "Atlantis.add_dynamic_polygons('/Users/simon/Documents/2022IMAS-OUC_SOMG/PracFiles/Atlantis2/Atlantis2_geometries.gpml')\n",
    "Atlantis.add_rotation_model('/Users/simon/Documents/2022IMAS-OUC_SOMG/PracFiles/Atlantis2/Atlantis2_rotations_rel.rot')\n",
    "\n",
    "final_grd_sampling = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d56d18-4574-4453-88ea-cf7fff5cb1a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### 2 run make mountain ranges\n",
    "\n",
    "reconstruction_model = Atlantis\n",
    "reconstruction_time = 0.\n",
    "min_distance_to_coastlines=0\n",
    "max_distance_to_trenches=1200000\n",
    "sampling = final_grd_sampling\n",
    "\n",
    "\n",
    "def get_paleogeography_rasters(reconstruction_model, reconstruction_time):\n",
    "    \n",
    "    rp = me.reconstruct_and_rasterize_polygons(reconstruction_model.continent_polygons[0], \n",
    "                                               reconstruction_model.rotation_model, \n",
    "                                               reconstruction_time=reconstruction_time, \n",
    "                                               sampling=sampling)\n",
    "\n",
    "    _, prox_ocean = me.raster_buffer(rp, inside='both')\n",
    "\n",
    "    # Get subduction zones, and compute a raster of distances to the nearest one\n",
    "    snapshot = reconstruction_model.plate_snapshot(reconstruction_time)\n",
    "\n",
    "    szs = snapshot.get_boundary_features(boundary_types=['subduction'])\n",
    "    all_sz_points = []\n",
    "    for sz in szs:\n",
    "        if sz.get_geometry():\n",
    "            all_sz_points.extend(sz.get_geometry().to_tessellated(np.radians(0.1)).to_lat_lon_list())\n",
    "\n",
    "    prox_sz = me.points_proximity(x=[lon for lat,lon in all_sz_points],\n",
    "                                  y=[lat for lat,lon in all_sz_points],\n",
    "                                  spacing=sampling)\n",
    "\n",
    "    # combine the rasters to isolate areas not too near to coastlines\n",
    "    # and not too far from subduction zones\n",
    "    m1 = prox_ocean.where((prox_ocean>min_distance_to_coastlines))\n",
    "    m2 = prox_sz.where(prox_sz<max_distance_to_trenches)\n",
    "    m2.data = m1.data+m2.data\n",
    "    #mountain_grid = m2\n",
    "\n",
    "    m2.data[np.isnan(m2.data)] = -999\n",
    "\n",
    "    mountain_core = proximity(m2, target_values=[-999], distance_metric='GREAT_CIRCLE')\n",
    "\n",
    "\n",
    "    land = prox_ocean.where(prox_ocean==0, 1)\n",
    "\n",
    "    #topography = (mountain_core/200)+(land*200)\n",
    "    #topography = topography.where(topography>0, np.nan)\n",
    "\n",
    "    return mountain_core, land\n",
    "\n",
    "mtn, lnd = get_paleogeography_rasters(Atlantis, 150.)\n",
    "\n",
    "lnd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264ac912-d682-4814-b394-3e192116fe3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "from scipy.interpolate import interpn\n",
    "\n",
    "def interpolate_values(dataarray, coordinates):\n",
    "    \"\"\"\n",
    "    Interpolate values from a 2D xarray DataArray at arbitrary coordinates using linear interpolation.\n",
    "\n",
    "    Parameters:\n",
    "    dataarray (xarray.DataArray): 2D DataArray with coordinates as longitude and latitude.\n",
    "    points (list of tuples): List of (longitude, latitude) coordinates where interpolation is desired.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Interpolated values at the specified points.\n",
    "    \"\"\"\n",
    "    # Extract the coordinates and data values from the DataArray\n",
    "    lon = dataarray['x'].values\n",
    "    lat = dataarray['y'].values\n",
    "    values = dataarray.values\n",
    "\n",
    "    # Create the grid of points from the coordinates\n",
    "    grid = (lat, lon)\n",
    "\n",
    "    points = [(row.geometry.x, row.geometry.y) for i,row in coordinates.iterrows()]\n",
    "    \n",
    "    # Perform the interpolation\n",
    "    interpolated_values = interpn(grid, values, points, method='nearest', bounds_error=False, fill_value=None)\n",
    "\n",
    "    return interpolated_values\n",
    "    \n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "def generate_random_coordinates(n_points, central_lat=20, lat_std=5, lon_range=(-200, 200)):\n",
    "    \"\"\"\n",
    "    Generate random coordinates on the surface of a sphere with latitude following a Gaussian distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    n_points (int): Number of random points to generate.\n",
    "    central_lat (float): Central latitude for Gaussian distribution (in degrees).\n",
    "    lat_std (float): Standard deviation for the Gaussian distribution (in degrees).\n",
    "    lon_range (tuple): Range of longitude values (in degrees), typically (0, 360).\n",
    "    \n",
    "    Returns:\n",
    "    list of tuples: List containing the generated (latitude, longitude) pairs.\n",
    "    \"\"\"\n",
    "    # Define the truncated normal distribution for latitude\n",
    "    lat_min, lat_max = central_lat - (lat_std*3), central_lat + (lat_std*3)  # 15 degrees above and below the central latitude\n",
    "    a, b = (lat_min - central_lat) / lat_std, (lat_max - central_lat) / lat_std\n",
    "    lat_distribution = truncnorm(a, b, loc=central_lat, scale=lat_std)\n",
    "    \n",
    "    # Generate latitudes\n",
    "    latitudes = lat_distribution.rvs(n_points)\n",
    "    \n",
    "    # Randomly assign half of the points to the southern hemisphere\n",
    "    southern_hemisphere = np.random.choice([True, False], size=n_points)\n",
    "    latitudes[southern_hemisphere] *= -1\n",
    "    \n",
    "    # Generate longitudes uniformly\n",
    "    longitudes = np.random.uniform(lon_range[0], lon_range[1], n_points)\n",
    "    \n",
    "    # Combine latitudes and longitudes\n",
    "    #coordinates = list(zip(longitudes, latitudes))\n",
    "    \n",
    "    df = gpd.pd.DataFrame(data={'Longitude': longitudes,\n",
    "                                'Latitude': latitudes})\n",
    "    return gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.Longitude, df.Latitude), crs=4326)\n",
    "    \n",
    "\n",
    "def generate_climate_sensitive_deposits(land_raster, central_lat, lat_std, n_points=100):\n",
    "\n",
    "    coordinates = generate_random_coordinates(n_points, central_lat=central_lat, lat_std=lat_std)\n",
    "\n",
    "    # Interpolate values at the specified points\n",
    "    coordinates['interpolated_values'] = interpolate_values(land_raster, coordinates)\n",
    "    #coordinates = coordinates.query('interpolated_values>0')\n",
    "    \n",
    "    return coordinates\n",
    "\n",
    "\n",
    "coordinates = generate_climate_sensitive_deposits(lnd, central_lat=90, lat_std=15, n_points=500)\n",
    "\n",
    "plt.scatter(coordinates.geometry.x, coordinates.geometry.y, c=coordinates['interpolated_values'], marker='.')\n",
    "plt.axis([-180,180,-90,90])\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a17d1aa-37e1-4fc7-9e11-364c0097b42b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "def generate_weighted_random_points(dataarray, n_points):\n",
    "    \"\"\"\n",
    "    Generate random points weighted by the values in a 2D xarray DataArray.\n",
    "\n",
    "    Parameters:\n",
    "    dataarray (xarray.DataArray): 2D DataArray with likelihood values at each grid node.\n",
    "    n_points (int): Number of random points to generate.\n",
    "\n",
    "    Returns:\n",
    "    list of tuples: List containing the generated (x, y) coordinates.\n",
    "    \"\"\"\n",
    "    # Normalize the DataArray values to probabilities\n",
    "    values = dataarray.values\n",
    "    probabilities = values / values.sum()\n",
    "\n",
    "    # Flatten the probabilities and generate the CDF\n",
    "    flattened_probabilities = probabilities.ravel()\n",
    "    cdf = np.cumsum(flattened_probabilities)\n",
    "\n",
    "    # Generate uniform random samples\n",
    "    random_samples = np.random.rand(n_points)\n",
    "\n",
    "    # Map the uniform random samples to the CDF\n",
    "    random_indices = np.searchsorted(cdf, random_samples)\n",
    "\n",
    "    # Convert the flat indices back to 2D indices\n",
    "    y_indices, x_indices = np.unravel_index(random_indices, dataarray.shape)\n",
    "\n",
    "    # Generate the corresponding x, y coordinates\n",
    "    x_coords = dataarray['x'].values[x_indices]\n",
    "    y_coords = dataarray['y'].values[y_indices]\n",
    "\n",
    "    # Combine x and y coordinates into tuples\n",
    "    #coordinates = list(zip(x_coords, y_coords))\n",
    "    \n",
    "    df = gpd.pd.DataFrame(data={'Longitude': x_coords,\n",
    "                                'Latitude': y_coords})\n",
    "    return gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.Longitude, df.Latitude), crs=4326)\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Create a sample DataArray\n",
    "data = np.random.rand(100, 100)  # Example data\n",
    "#dataarray = xr.DataArray(data, dims=['y', 'x'], coords={'y': np.linspace(-50, 50, 100), 'x': np.linspace(-50, 50, 100)})\n",
    "\n",
    "n_points = 1000\n",
    "random_points = generate_weighted_random_points(mtn, n_points)\n",
    "\n",
    "# Print a few sample coordinates\n",
    "#for coord in random_points[:10]:\n",
    "#    print(coord)\n",
    "plt.plot(random_points.geometry.x, random_points.geometry.y, '.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d149e31-51ff-4947-ae35-d2b6be284be3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extended_plate_polygons = gpml2gdf(pygplates.FeatureCollection('/Users/simon/Documents/2022IMAS-OUC_SOMG/PracFiles/Atlantis2/Atlantis2_extended_continents.gpml'))\n",
    "\n",
    "def unreconstruct_geodataframe(gdf, reconstruction_model, reconstructed_polygons, reconstruction_time):\n",
    "\n",
    "    gdf = gdf.overlay(reconstructed_polygons, how='intersection', keep_geom_type=False)\n",
    "    return reconstruction_model.reconstruct(gdf, reconstruction_time=reconstruction_time, reverse=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587dd4c4-aeab-4648-900a-acbfe76836ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reconstruction_model = Atlantis\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "for reconstruction_time in np.arange(200,-1,-20):\n",
    "    \n",
    "    print(reconstruction_time)\n",
    "    mountain_core, land = get_paleogeography_rasters(reconstruction_model, reconstruction_time)\n",
    "\n",
    "    reconstructed_polygons = reconstruction_model.reconstruct(extended_plate_polygons, reconstruction_time)\n",
    "\n",
    "    # Evaporites\n",
    "    gdf = generate_climate_sensitive_deposits(land, central_lat=20, lat_std=5, n_points=50)\n",
    "    ur_gdf = unreconstruct_geodataframe(gdf, reconstruction_model, reconstructed_polygons, reconstruction_time)\n",
    "    ur_gdf.plot(ax=ax, color='orange', markersize=2)\n",
    "    \n",
    "    # Glacial Deposits\n",
    "    gdf = generate_climate_sensitive_deposits(land, central_lat=90, lat_std=15, n_points=100)\n",
    "    ur_gdf = unreconstruct_geodataframe(gdf, reconstruction_model, reconstructed_polygons, reconstruction_time)\n",
    "    if ur_gdf is not None:\n",
    "        ur_gdf.plot(ax=ax, color='blue', markersize=5)\n",
    "    \n",
    "    # Volcanos\n",
    "    volcano_points = generate_weighted_random_points(mountain_core, n_points=20)\n",
    "    ur_gdf = unreconstruct_geodataframe(volcano_points, reconstruction_model, reconstructed_polygons, reconstruction_time)\n",
    "    ur_gdf.plot(ax=ax, color='darkred', markersize=2)\n",
    "\n",
    "    #break\n",
    "\n",
    "plt.axis([-180,180,-90,90])\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc26243-6fc9-4571-bb59-3419d6e13345",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "land.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f0f15a-fe37-422a-9890-b61a5905e207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pygmt10]",
   "language": "python",
   "name": "conda-env-pygmt10-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
